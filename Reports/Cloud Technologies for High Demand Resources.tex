\documentclass{llncs}
\usepackage{graphicx,amssymb,pepa}

\begin{document}
\title{Investigating Cloud Technologies to Maximise Availability of Oversubscribed Resources}

\author{Stephen Shephard}

\institute{School of Computing Science, Newcastle University, Newcastle upon Tyne, NE1 7RU\\
	\email{s.shephard2@newcastle.ac.uk}}

\maketitle

\begin{abstract}
On-line Transaction Processing (OLTP) applications must frequently deal with the issue of skewed demand for some resources.  This demand may overwhelm the whole system, affecting the owner's reputation and revenue.  In this article we present a ticketing use case and argue that at each layer of the architecture, the distributed computing technologies of the Cloud may maintain throughput to the lower demand resources, maximising the available functionality of the system.  
	\keywords{Cloud, middleware, microservices, distributed databases, load-balancing, performance}
\end{abstract}

\section{Introduction}

There are many high-profile examples of whole IT systems brought down by customer demand for part of their services.  Customers were prevented from using any part of the London 2012 Olympic ticketing website on launch day to avoid demand overloading the system \cite{telegraph2011olympics}.  HBO Go was brought down by demand for the finale of ``True Detective'' \cite{hbo2014}.  Apple's iTunes Store suffered outage on the launch day of the iPhone 7 (new iPhone registration is carried out via an iTunes function) \cite{itunes2016}.

I propose that it is possible to design and build more resilient systems through effective use of Cloud technologies where higher than normal demand for one function or type of resource would not block access to the others. The London 2012 Olympics have now passed into history, but such a ticketing system would have many applications today, and may be generalised to a system for allocating and releasing other resources with variable demand.

\section{Background}

I will consider an example system based on the Olympic ticketing use case above.  Tickets will be for a multi-sport event, and each will consist of a ticket type (the sport), date, row, and seat number.  The system will support three operations:
\begin{enumerate}
\item Search (for available tickets)
\item Book (allocate a ticket to a customer)
\item Return (customer releases a ticket allocation)
\end{enumerate}

If it is not possible to search for or book tickets of one type because some component is overloaded due to demand, then the system should allow booking of other ticket types.  It must also always be possible to return tickets of any type.

In this article, I am considering the problem of high demand for types of ticket.  This is not about the number of tickets available and I will not consider issues of fair allocation of scarce resources.  The demand may be:
\begin{enumerate}
\item predictable - we know which functions or ticket types are going to have the highest demand; or
\item unknown - we discover the areas of highest demand once the application goes online.
\end{enumerate}

\paragraph{System Architecture.}

The proposed system will use a distributed architecture.  Users will access it from a web-based front end.  Tickets will be stored in a database partitioned across several data nodes.  In between the web servers and database will be a number of worker applications to service user requests, connected to the web servers by some middleware.

\begin{figure}
\caption{Ticketing application distributed architecture}
\centering
\includegraphics[trim = 5 5 5 5, clip, width=\textwidth]{application}
\end{figure}

\section{Technology Review}

We consider current cloud technologies that may be useful in distributing throughput generated by high demand throughout the system, and in decoupling its components from each other.

\subsection{HTTP Load Balancing}

HTTP load balancing improves the scalability of a web-based application by distributing the demand across multiple web servers.  It also improves the availability of the overall system by directing HTTP requests to other servers if a web server fails.  It therefore plays an essential role in a high-throughput system where an individual web server could not meet the demand.

\subsubsection{Content-blind load balancing.}  Content-blind load balancing is unaware of the information contained in the HTTP requests.  It can be carried out wholly within layers 2 and 3 of the OSI reference model \cite{day1983osi} (Data Link or Network) according to one of the following algorithms \cite{gilly2011up}.

\paragraph{Round Robin.} Assign connections sequentially on a simple round-robin basis.  In a homogeneous environment this may be unweighted - each server receives equal connections over time.  With heterogeneous web servers, a weighted version of the algorithm distributes the percentage of traffic proportionately to the different capacities of the servers.

\paragraph{Least Connection.}  This is a dynamic scheduling algorithm that assigns each incoming connection to the web servers with the least number of current connections. Again, this may be weighted or unweighted.

\paragraph{Least Loaded.} Assign connections to the web server that has the highest spare capacity, determined by an agent on each server that updates the load balancer on its capacity and utilisation.

\paragraph{Random Server Selection.}  Assign connections to web servers according to a uniform random sequence.

\subsubsection{Content-aware load balancing.} Content-aware load balancing can act based on the information contained in the HTTP requests.  Routing may therefore depend on URIs identifying different parts of a web site or RESTful services, or even certain query parameters or POST payloads.  In our use case this gives us a means of {\itshape isolating} demand for certain services rather than balancing it across all web servers.

Furthermore, as HTTP/1.1 permits persistent connections, several HTTP requests from a client may use the same TCP connection.  This reduces server and network overheads resulting in a lower response time.

Content-aware load balancing must take place at layer 7 of the OSI model (Application i.e. HTTP).  In most content-aware architectures, the load balancer forwards the HTTP response from the targeted web server back to the client.  This makes the load balancer a bottleneck in the system.

\subsubsection{Evaluation.}
Response time and web server utilisation are useful metrics for comparing different load balancing techniques \cite{bryhni2000comparison}.  An effective HTTP load balancer will produce a low response time, representing a desirable user experience, and evenly distributed utilisation of each web server, minimising the cost to the system owner.

\subsection{Elastic Scaling}

{\itshape Rapid elasticity} is an essential characteristic of Cloud Computing by the NIST definition \cite{mell2011nist}.  Computing resources, in our ticketing system web servers, worker applications and even data nodes, can be elastically and often automatically scaled to meet current demand.  This gives the appearance of resources that are limited only by the system owner's budget.

Amazon Web Services \cite{awsautoscaling} and Microsoft Azure \cite{azurescalesets} both offer autoscaling products.  They have similar capabilities and I will consider the Amazon Web Services offering in more detail.

\subsubsection{AWS Auto Scaling.} In Amazon Web Services Auto Scaling, the system designer defines a logical group of EC2 instances (e.g. Web servers) with a minimum and maximum number of servers, and a Scaling Plan.  This grouping of instances is a means of sharing load.  Load may be isolated between parts of the system by organising them into different autoscaling groups.

Manual Scaling and Scheduled Scaling are available but most relevant for our use case is Dynamic Scaling, based on demand.  An alarm is raised when the value of a metric breaches a threshold for a defined period.  A policy then instructs Auto Scaling whether to respond to an alarm with single adjustments per alarm or adjustments based on the size of the threshold breach.

\paragraph{Scaling based on metrics.} Alarms based on metrics from Amazon CloudWatch may trigger auto scaling.  Status, Disk, Network and CPU Utilisation metrics are currently available. These may be measured per EC2 instance or statistically aggregated over the Auto Scaling group.  For example, a scaling policy may provision an additional Web server when the average Web server CPU Utilisation rises above 80\% in a system with uniform load balancing.

\paragraph{Scaling based on Amazon SQS.} Amazon SQS is a message queueing system (see {\itshape Middleware}).  Amazon Cloudwatch may also collect metrics on queues, the number of messages sent, received and deleted, the age of the oldest message in the queue and also the length of the queue i.e. the number of messages awaiting retrieval.  The queue length increases with demand and decreases as a system services that demand, so a scaling policy may provision an additional Worker Application when the length of the queue supplying it breaches a threshold.

\paragraph{Launch Configurations and Lifecycle Hooks.} In a scenario where a hard maximum of servers is set, say limited by budget, it would be possible to pre-provision all the EC2 instances that may be required and switch them on and off with demand.  For genuinely elastic scaling, it must be possible to provision instances dynamically.  A {\itshape Launch Configuration} in AWS is a template for creating a new EC2 instance of a required configuration.

This may be sufficient for auto-scaling basic Web servers; there are off the shelf Amazon Machine Images available for the major web servers.  For the Worker Applications however, we need to install bespoke software for our ticketing system on each instance.  This is achieved via {\itshape Lifecycle Hooks} that call custom actions on instance launch or shutdown.  Of course, this means it takes time after provisioning an additional Worker Application instance before it is usable.   We need to factor this into the metric threshold and specify a cooldown period before the alarm triggers again.

\subsubsection{Evaluation.}
An effective elastic scaling strategy will minimise both underutilisation and overutilisation of compute resources.  Underutilisation indicates over-provision of instances, which is inefficient in energy usage and uneconomic in cost.  Overutilisation may cause poor response time and even outage leading to loss of revenue.
\cite{espadas2013tenant}

\subsection{Distributed databases}
Modern databases both SQL and NoSQL are designed to scale both data and the load of operations accessing that data over many servers that do not share disk or RAM, so-called ``shared nothing'' architecture \cite{cattell2011scalable}.  We may partition data {\itshape vertically}, dividing tables into groups of columns that may be placed on different data nodes; or {\itshape horizontally}, where the split is by row \cite{agrawal2004integrating}. 

In our use case, the quantity of data does not approach the levels of ``Big Data'' applications.  We are interested in partitioning as a means of scaling the demand for that data.  Our ticketing system will not require a large number of columns and the three operations outlined do not have significantly different column requirements.  Horizontal partitioning is most relevant.  The partition key of a Ticket table may be the Ticket Type, the Date, or the seat Row.  Demand for tickets is likely to vary by each of these attributes.  An alternative partitioning strategy would be to on denormalised tables supporting the query, book and return operations.  The load on each data node would follow the demand for the data types and operations placed there.

The scalability of distributed databases usually comes at the price of a relaxed consistency model - so-called BASE (Basically Available, Soft state, Eventually Consistent) rather than ACID (Atomic, Consistent, Isolated, Durable) transactions.  In our ticketing system, eventual consistency is clearly sufficient for the return ticket scenario - returned tickets do not have to be made immediately available for booking.  Individual ticket bookings must exist on only one partition to prevent the same ticket being booked more than once.  Eventual consistency between search and book operations requires the customer to tolerate the concept of ``reservation'' of a ticket for a short period until a booking can be confirmed \cite{microservicesdata}\cite{cattell2011scalable}.

Another issue to be aware of is {\itshape replication}.   Most distributed databases offer replication of data from one partition to another for availability.  In our use case, if a data node is overloaded by demand, the system may failover to a copy of the data on another data node, but this will just transfer the demand elsewhere.  If this is also the primary data node of an otherwise low demand data type, then it may be overwhelmed in turn.

Where the high demand is unknown in advance, we need an adaptive strategy.  Workload-aware clustering algorithms do exist for the placement of new data, e.g. \cite{kamal2016workload}, but our use case has a fixed set of tickets.  Re-placement of existing data onto different partitions would be likely to require many reads, writes and deletes.

\begin{figure}
\caption{Distributed database, without and with replication}
\centering
\includegraphics[trim = 5 5 5 5, clip, width=\textwidth]{dbdist}
\end{figure}

\subsubsection{Evaluation.}

A successful partitioning strategy will ensure that an individual operation only uses a single data node; that every data node is equally used when demand is evenly distributed; and that any impact from skewed demand is limited to the directly affected data node.

\subsection{Middleware}

Good choice of middleware in our system will help ensure our components are connected, but loosely coupled.  If, for example, a web server is blocked waiting for a response from a worker application carrying out a more expensive operation, then the throughput of the web server will be limited to that of the worker application.  Also, failure of one of the processes in a distributed system may cause failure of the system as a whole.

\paragraph{Synchronous vs Asynchronous Middleware.}
With synchronous middleware such as Remote Procedure Call (RPC), the calling process is blocked until the called service completes and returns control to the caller.  The system components are tightly coupled.  This is undesirable for our ticketing application.

Distributed systems using some form of asynchronous middleware do not block when calling a remote service.  Control is immediately passed back to the caller, and a response may be returned eventually, with the caller polling the remote service for the response, or the remote process calling a method in the caller to send the response.

The ``return'' operation use case does not require a direct response from the system.  As long as the customer can rely on eventual guaranteed delivery of the return request, (and that the cost of their ticket will be refunded) then they do not need to wait for a direct response to their return.

\subsubsection{Message-Oriented Middleware (MOM).}  MOM is a form of Asynchronous Middleware, commonly provided by Larger Cloud service providers such as Amazon Web Services and Microsoft Azure.  These brokered message services provide an intermediate layer between senders and receivers, decoupling their communication.  Message delivery may take minutes rather than milliseconds, but the service providers do provide configurable delivery guarantees \cite{curry2004message}.

There are two main messaging models, both of which are offered by Microsoft Azure Service Bus \cite{azureservicebus} for example.

\paragraph{Point-to-Point Queues.} Azure Queues are a point-to-point service implementing First In, First Out (FIFO) message delivery. Many processes may send messages to a queue, and each message is received by one consumer - though it may be one of several consumers competing for messages from this queue.  This competing consumer pattern offers a means of balancing load from our Web servers between our Worker Applications.

\paragraph{Publish/subscribe.}  Publish/subscribe (in Azure, topics and subscriptions) are a properly one-to-many or many-to-many communication mechanism.  Any single producer may send one message to a topic, and then all consumers that subscribe to that topic receive a copy of the same message.

\subsubsection{Evaluation.}
The MOM services from Microsoft and Amazon provide tools for monitoring the number of messages in a queue or topic, and middleware can be examined by measuring whether a queue size reaches a steady state, or grows faster than downstream services can consume the messages.  However, this is affected more by the services using the middleware than the choice of middleware itself.  This is likely to be best achieved by end to end measurement.

\subsection{Microservices}

Microservice architecture is an approach to structuring applications as suites of small services, defined by business capability verticals rather than technological layers  \cite{lewis2014microservices} \cite{microservicesverticals}.  Each of our use case requirements - search for tickets, book tickets, return tickets - might be microservices with their own worker applications and data nodes.  Ticket data would be denormalised across the data nodes and made eventually consistent via a backplane messaging service \cite{microservicesdata}.  This would certainly isolate the demand for search, book and return from each other - returning tickets would not be blocked by a system where booking tickets was overloaded.  We would need a lower level of granularity however to deal with skewed demand for a particular type of ticket, perhaps a separate microservice for booking each type. 

\begin{figure}
\caption{Microservices}
\centering
\includegraphics[trim = 5 5 5 5, clip, width=\textwidth]{microservices}
\end{figure}

\subsubsection{Evaluation.}
As for middleware, evaluation of different microservices approaches depends on end to end measurement.   However, one area of interest is the efficiency of highly granular microservices.  If the demand for the microservices is isolated then it is possible that some low demand services are underutilised.

\section{Modelling}

In our presentation of technologies relevant to the ticketing system use case, we have discussed criteria to evaluate each of them individually.  In a distributed system we need to examine the end to end impacts.  What are the optimum relative numbers of Web servers and Worker Applications for a given demand? Does removing a bottleneck at the Web or Worker Application layer shift the stress to the middleware, or the database?  If we accept that some levels of demand cannot be met on a limited budget, and that some components will no longer meet the required throughput, how do we determine the impact on the remainder of the system?

\paragraph{Process Algebra.} Process Algebras (such as PEPA or TIPP \cite{gotz1993multiprocessor}) allow us to model throughput in interdependent processes, with a mixture of independent and shared actions operating at different rates.  Each of our components can be described in this way, and queues have already been extensively modelled in PEPA \cite{thomas1997using}.  The nature of process algebra as a mathematical language also means that it is possible to build a model of a whole system by composition of the component models.

\begin{figure}
\caption{PEPA queue model}
\centering
% Automatically generated by PEPA2Latex
% --begin
\begin{displaymath}
	\begin{array}{rcl}
%[0.0ex]		
\mathit{Web} & \rmdef & (\mathit{request},\mathit{r}).\mathit{Web}\\
		\mathit{Worker} & \rmdef & (\mathit{service},\mathit{s}).\mathit{Worker}\\
		\mathit{Queue_{0}} & \rmdef & (\mathit{request},\mathit{r}).\mathit{Queue_{1}}\\
		\mathit{Queue_{1}} & \rmdef & (\mathit{service},\mathit{s}).\mathit{Queue_{0}}\\
[0.0ex]		\multicolumn{3}{l}{\mathit{Web}\sync{request}\mathit{Queue_{0}}[N]\sync{service}\mathit{Worker}}\\
[0.0ex]	\end{array}
\end{displaymath}
% --end
\end{figure}

\paragraph{CloudSim.}  CloudSim \cite{calheiros2011cloudsim} is a Java framework for developing cloud datacentre simulations.  Much of it is concerned with modelling the efficient running of that infrastructure, for example the power usage, but it also includes utilisation models and may be useful for predicting the effect of elastic scaling.

CloudSim simulations require Java development for creation and modification, which is an overhead in building the models but offers more flexibility in applying them.  Process Algebra has closed-form solutions, though there is a PEPA Workbench tool \cite{gilmore1994pepa} that allows PEPA specifications to be parsed and run like programs, aiding experimentation on a range of action rates by automating repetitive calculations.  Both currently have their place as they predict different quantities of interest.

\section{Conclusion and Future Work}
In this article we showed that cloud technologies could manipulate the throughput at each of the layers of our ticketing system architecture.
\paragraph{Front-end.}  We can balance demand at the web front-end using content-blind HTTP load balancing, and isolate skewed demand using content-aware algorithms.  Elastic scaling of web servers enables the front-end to respond to as much demand as the system owner is willing to pay for.
\paragraph{Application.} We can decouple worker applications from the front-end using asynchronous middleware.  Shared middleware balances the load, microservice architecture isolates it.  The system can adapt to current demand by using elastic scaling to create or destroy worker applications, and by using scaling groups we can ensure that the number of each application type is appropriate to the demand.
\paragraph{Database.} With care, we can use horizontal database partitioning to ensure that functions and/or data types are not shared between data nodes, isolating their demand from each other.

At the component level we can see whether an approach will balance or isolate load, or adapt to it, but at the system level we will need modelling techniques to predict the end to end throughput.  We looked at two approaches, process algebra and programmatic, that could be used to build complex models from smaller components.

An interesting area of future work would be to create a test system based on Red Hat's Ticket Monster application \cite{redhatticketmonster} and build it both as a model in PEPA, and an instrumented system running alongside it on real infrastructure. The model's predictions could then be tested against measurements of the real system.  Both model and real system could be deployed in different configurations of interest - balancing demand via shared middleware or isolating it via the microservices approach - to see how each copes with different demand scenarios.  For example, with rising skewed demand for one ticket type, at what point does the balanced demand approach begin to affect the entire system?  Do either of the shared middleware and microservices approaches have clear efficiency advantages under certain conditions?

A further area might be in using the modelling techniques as adaptive algorithms.  A CloudSim simulation might be used as a policy for elastic scaling, and compared with the performance of other right-sizing strategies; control theory, machine learning and other model based techniques including statistical.

%
% ---- Bibliography ----
%

\bibliographystyle{splncs03}
\bibliography{project}

\end{document}