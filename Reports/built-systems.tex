%
% ---- Built systems
%

\section{Built systems}\label{sec:built-systems}

\subsection{Approach}

To test the system models, real systems using those architectures are built and their throughputs measured under simulated workloads.

\subsubsection{Designs.}  All three systems were developed in Java and deployed on Microsoft Azure virtual machines.  The specifics of each design are discussed by system, but the common feature is that all three systems use a Cassandra database or databases with the same ticket schema design, given in Figure \ref{figure:cassandra_ticket_schema}, with 500 tickets of each sport.

Cassandra was chosen as the distributed database models were designed for consistent hashing (most relevant for the model using replication).  Although the simple microservices model need not have used Cassandra databases, reusing the schema design and database servers was an economical choice, and it illustrates how different architectures may arrange many of the same components.

The source code for the systems and related tools is hosted on GitHub \cite{RN1073}.

\begin{figure}
	\caption{Cassandra database ticket schema}
	\label{figure:cassandra_ticket_schema}
	\centering
	\begin{lstlisting}
	/*
	* Ticket table schema
	* 
	* int id - unique ticket id
	* varchar sport - type of sport
	* int day - day of event
	* int seat - seat number
	* varchar owner - name of booked ticket owner
	* 
	* The partition key is sport
	* The clustering columns are owner, day, id
	*/
	
	CREATE TABLE IF NOT EXISTS ticket (
		id int,
		sport varchar,
		day int,
		seat int,
		owner varchar,
		PRIMARY KEY (sport, owner, day, id)
	) WITH comment = 'Tickets';
	\end{lstlisting}
\end{figure}

\subsubsection{Workload Simulation.}  The workload from a web application and its users was simulated using Apache JMeter \cite{RN1074}, a Java application designed for load testing.  JMeter was originally designed for testing web applications and all the systems have RESTful APIs over HTTP (for the simple microservices architecture, the Java Spring APIs; for the shared queue architectures, the Microsoft Azure Storage Queue REST APIs).  JMeter test plans are composed of thread groups, where the number of threads and test executions are specified, samplers such as HTTP Request, and timers to determine the delay between each test execution.  The required demand may be increased by increasing the number of threads used in the test.  JMeter's Poisson Random Timer is used to simulate the negative exponential distribution \cite{RN80} that PEPA steady state solutions assume.

\subsubsection{Measurement.}  Measurement of throughputs in the built systems is required to produce results that can be compared with the PEPA model experimental results.

\paragraph{Measurands.} The measurands are the throughputs of athletics and cycling requests in the worker applications, and control requests that do not access the database in order to show any system overheads.  Where applicable, measurands include the throughputs of diving requests and database queries.

\paragraph{Measurement method.}  The worker applications are measured using Coda Hale Metrics \cite{RN1079}, a Java library providing instrumentation.  The instrument used is a {\itshape Meter}, that measures the rate at which events occur as mean and moving average rates.  Database throughputs are measured by enabling the built-in Cassandra metric ThreadPools.CompletedTasks.request.ReadStage (the total count of completed read queries).  In both cases the metrics are logged every 10 seconds.

\paragraph{Measurement procedure.} The largest 1-minute moving average over a test run is extracted by a Python script.  For the worker applications, the 1-minute average is provided by the Meter instrument.  For Cassandra metric files, the average must be calculated, again using a Python script.  Each experiment is carried out five times and the mean is taken of the five sets of results.

%
% ---- Simple microservices
%
\FloatBarrier
\subsection{Simple microservices}

Figure \ref{figure:simplemicro}

\begin{table}[h!]
	\begin{center}
		\caption{Simple microservices Azure VMs}
		\label{table:builtmicro_vmdesign}
		\begin{tabular}{l | l | l}
			VM		& Specification		& Application \\
			\hline
			worker1	& DS1 v2 (1 core, 3.5 GB)	& SimplemicroApplication \\
			worker2	& DS1 v2 (1 core, 3.5 GB)	& SimplemicroApplication \\
			db1		& F1s (1 core, 2 GB)	& Cassandra \\
			db2		& F1s (1 core, 2 GB)	& Cassandra \\
		\end{tabular}
	\end{center}
\end{table}

Two completely separate Cassandra databases, each on an Azure Standard F1s (1 core, 2 GB memory) Ubuntu Virtual Machine, one containing Athletics tickets, one containing Cycling tickets.

Two worker applications using RESTful APIs using Java Spring \cite{RN1076}.  Each worker application runs on a separate Azure Standard DS1 v2 (1 core, 3.5 GB memory) Ubuntu Virtual Machine.  Each connects to one of the Cassandra databases.

\begin{shaded}
Each worker application has a \\control API which doesn't access the database, but for which metrics are recorded.
Each worker application also has a \\search API which takes a sport parameter (Athletics or Cycling) and queries the database for all matching tickets.

Use JMeter with Poisson random timer (negative exponential distribution) with a lambda value of 500 milliseconds, Cycling at a constant 10 threads/users (approximately 20 requests per second) and Athletics ramping up from 10-100 in steps of 10, so the desired demand is 20-200 requests per second.  Each thread group has a loop count of 500 requests, ensuring several minutes worth of samples and therefore a number of rolling 1-minute averages.

The control version of the above JMeter test plan uses the same variables, but sends both Athletics and Cycling requests to the \\control API.

Limit of throughput using \\search was measured at 130 queries per second.  This is lower than the throughput suggested by using the cassandra-stress tool, and suggests that using Java Spring Data adds overheads to the database requests (most likely, as the RESTful requests to the worker applications are sessionless, this is starting a new database session for each request).

See the experimental results in Table \ref{table:builtmicro_results}.

Control shows that throughput approaches demand (difference likely to be due to random distribution, network latency, etc).  However the Athletics demand is throttled by the database throughput.  The Cycling throughput is unaffected by the Athletics demand. 
\end{shaded}
\begin{table}[h!]
	\begin{center}
		\caption{Simple microservices experimental results}
		\label{table:builtmicro_results}
		\pgfplotstabletypeset[
		col sep=comma,
		string type,
		columns/ausers/.style={column name=users, column type={p{.1\textwidth}}},
		columns/arate/.style={column name=rate, column type={p{.1\textwidth}}},
		columns/athletics/.style={column name=search, column type={p{.1\textwidth}}},
		columns/acontrol/.style={column name=control, column type={p{.1\textwidth}}},
		columns/cusers/.style={column name=users, column type={p{.1\textwidth}}},
		columns/crate/.style={column name=rate, column type={p{.1\textwidth}}},
		columns/cycling/.style={column name=search, column type={p{.1\textwidth}}},
		columns/ccontrol/.style={column name=control, column type={p{.1\textwidth}}},
		every head row/.style={before row=\hline \multicolumn{4}{c}{Athletics} & \multicolumn{4}{c}{Cycling} \\,after row=\hline},
		every last row/.style={after row=\hline},
		]{data/builtmicro/results.csv}
	\end{center}
\end{table}

\begin{figure}
	\caption{Simple microservices experimental results}
	\label{figure:builtmicro_charts}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	title={Throughput against athletics demand},
	xlabel={Athletics demand},
	ylabel={Throughput},
	xmin=0, xmax=200,
	ymin=0, ymax=150,
	legend pos=north west,
	ymajorgrids=true,
	grid style=dashed,
	cycle multiindex* list={
		mark list*
		\nextlist
		cyan,brown,green,blue,red
	}
	]
	
	\addplot table [x index={1}, y index={2}, col sep=comma]{data/builtmicro/results.csv};
	\addplot table [x index={1}, y index={6}, col sep=comma]{data/builtmicro/results.csv};
	
	\legend{athletics,cycling}
	
	\end{axis}
	\end{tikzpicture}
\end{figure}

%
% ---- Shared queue middleware
%
\FloatBarrier
\subsection{Shared queue and distributed database}

\begin{table}[h!]
	\begin{center}
		\caption{Shared queue and distributed DB Azure VMs}
		\label{table:builtddnr_vmdesign}
		\begin{tabular}{l | l | l}
			VM		& Specification			& Application \\
			\hline
			qworkers	& DS3 v2 Promo (4 cores, 14 GB)	& QueueWorker (16 threads) \\
			db1		& F1s (1 core, 2 GB)		& Cassandra \\
			db2		& F1s (1 core, 2 GB)		& Cassandra \\
		\end{tabular}
	\end{center}
\end{table}

\begin{shaded}
This would normally be used for the return ticket operation.  To ensure that a usable Cassandra metric was available, a search operation was used again.

Distributed Cassandra databases using two nodes each on an Azure Standard F1s (1 core, 2 GB memory) Ubuntu Virtual Machine, using a Distributed keyspace with SimpleStrategy, replication\_factor=1.  Cassandra's partitioning places Athletics tickets on one node, and Cycling tickets on the other.  This is validated using nodetool getendpoints e.g.

nodetool getendpoints Distributed ticket Athletics

Cassandra configured to record metrics of a count of all completed read queries every 10s i.e.

org.apache.cassandra.metrics.ThreadPools.CompletedTasks.request.ReadStage

A Python script calculates the rolling 1-minute average throughput from these counts.

A single shared Azure Storage Queue is used.

A single multithreaded QueueWorker application dequeues every request from the shared Azure Storage Queue.  It runs on an Azure Standard DS3 v2 Promo (4 cores, 14 GB memory) Ubuntu Virtual Machine.  As populating the queue with JMeter and processing it with the worker application are decoupled, it was possible to run QueueWorker on a prepopulated queue to determine its maximum throughput i.e. regardless of the incoming demand. Using this technique suggested that maximum performance came with QueueWorker running with 16 threads.

Note that without the overheads of starting a new Cassandra database session for each request, it is necessary to slow Cassandra down by turning on tracing for 100\% of queries - bin/nodetool settraceprobability 1.0

QueueWorker processes both Control tickets and real (Athletics, Cycling) tickets.  Metrics are recorded for all requests, but for real ticket requests a database select of all tickets for the matching sport is carried out first and the metric is only recorded if the query returns results.

Use JMeter with Poisson random timer (negative exponential distribution) with a lambda value of 100 milliseconds, Cycling at a constant 15 threads/users (approximately 95 requests per second) and Athletics ramping up from 15-150 in steps of 15, so the desired demand is 95-950 requests per second.  Each thread group has a loop count of 1500 requests, ensuring several minutes worth of samples and therefore a number of rolling 1-minute averages.

The control version of the above JMeter test plan uses the same variables, but sending Control tickets rather than Athletics and Cycling tickets to the queue.

Does the predicted result, that the queue will be the overriding constraint, still hold true with a real, effectively unlimited queue?

See the experimental results in Table \ref{table:builtddnr_results}.

\end{shaded}

\begin{table}[h!]
	\begin{center}
		\caption{Shared queue with distributed DB experimental results}
		\label{table:builtddnr_results}
		\pgfplotstabletypeset[
		col sep=comma,
		string type,
		columns/conusers/.style={column name=users, column type={p{.1\textwidth}}},
		columns/conrate/.style={column name=rate, column type={p{.1\textwidth}}},
		columns/ausers/.style={column name=users, column type={p{.1\textwidth}}},
		columns/arate/.style={column name=rate, column type={p{.1\textwidth}}},
		columns/cusers/.style={column name=users, column type={p{.1\textwidth}}},
		columns/crate/.style={column name=rate, column type={p{.1\textwidth}}},
		columns/db1/.style={column name=db1, column type={p{.1\textwidth}}},
		columns/db2/.style={column name=db2, column type={p{.1\textwidth}}},
		every head row/.style={before row=\hline \multicolumn{2}{c}{Control} &
		\multicolumn{2}{c}{Athletics} & \multicolumn{2}{c}{Cycling} &
		\multicolumn{2}{c}{Database} \\,after row=\hline},
		every last row/.style={after row=\hline},
		]{data/builtddnr/results.csv}
	\end{center}
\end{table}

\begin{figure}
	\caption{Shared queue with distributed DB experimental results}
	\label{figure:builtddnr_charts}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	title={Throughput against athletics demand},
	xlabel={Athletics demand (users)},
	ylabel={Throughput},
	xmin=0, xmax=150,
	ymin=0, ymax=1000,
	legend pos=north west,
	ymajorgrids=true,
	grid style=dashed,
	cycle multiindex* list={
		mark list*
		\nextlist
		cyan,brown,green,blue,red
	}
	]

	\addplot table [x index={2}, y index={3}, col sep=comma]{data/builtddnr/results.csv};
	\addplot table [x index={2}, y index={5}, col sep=comma]{data/builtddnr/results.csv};
	\addplot table [x index={2}, y index={1}, col sep=comma]{data/builtddnr/results.csv};
	
	\legend{athletics,cycling,control}
	
	\end{axis}
	\end{tikzpicture}
\end{figure}

%
% ---- Distributed database with replication
%
\FloatBarrier
\subsection{Shared queue and distributed database with replication}


\begin{table}[h!]
	\begin{center}
		\caption{Shared queue and distributed DB with replication Azure VMs}
		\label{table:builtddwr_vmdesign}
		\begin{tabular}{l | l | l}
			VM		& Specification			& Application \\
			\hline
			qworkers	& DS3 v2 Promo (4 cores, 14 GB)	& QueueWorker (16 threads) \\
			db1		& F1s (1 core, 2 GB)		& Cassandra \\
			db2		& F1s (1 core, 2 GB)		& Cassandra \\
			db3		& F1s (1 core, 2 GB)		& Cassandra \\
		\end{tabular}
	\end{center}
\end{table}

\begin{shaded}
Distributed Cassandra databases using three nodes each on an Azure Standard F1s (1 core, 2 GB memory) Ubuntu Virtual Machine, using a Replicated keyspace with SimpleStrategy, replication\_factor=2.  Cassandra's partitioning places Athletics, Cycling and Diving tickets on different nodes with each node also containing replicas of one other ticket type.  Note that it was necessary to use ByteOrderedPartitioner to force this.

This is validated using nodetool getendpoints e.g.

nodetool getendpoints Distributed ticket Athletics

Athletics/Cycling
Cycling/Diving
Diving/Athletics

Cassandra metrics configured and processed as above.

The same shared Azure Storage Queue and QueueWorker application running with 16 threads used again.

Cassandra with tracing for 100\% of queries as before.

Use JMeter with Poisson random timer (negative exponential distribution) with a lambda value of 100 milliseconds, Cycling and Diving at a constant 15 threads/users (approximately 95 requests per second) and Athletics ramping up from 15-150 in steps of 15, so the desired demand is 95-950 requests per second.  Each thread group has a loop count of 1500 requests, ensuring several minutes worth of samples and therefore a number of rolling 1-minute averages.

The control version of the above JMeter test plan uses the same variables, but sending Control tickets rather than real tickets to the queue.

See the experimental results in Table \ref{table:builtddwr_results}.
\end{shaded}

\begin{table}[h!]
	\begin{center}
		\caption{Shared queue and distributed database with replication experimental results}
		\label{table:builtddwr_results}
		\pgfplotstabletypeset[
		col sep=comma,
		string type,
		columns/ausers/.style={column name=(users), column type={p{.1\textwidth}}},
		columns/arate/.style={column name=athletics, column type={p{.1\textwidth}}},
		columns/crate/.style={column name=cycling, column type={p{.1\textwidth}}},
		columns/drate/.style={column name=diving, column type={p{.1\textwidth}}},
		columns/total/.style={column name=total, column type={p{.1\textwidth}}},
		columns/conrate/.style={column name=control, column type={p{.1\textwidth}}},
		columns/ratio/.style={column name=ratio, column type={p{.1\textwidth}}},
		columns/db1/.style={column name=db1, column type={p{.1\textwidth}}},
		columns/db2/.style={column name=db2, column type={p{.1\textwidth}}},
		columns/db3/.style={column name=db3, column type={p{.1\textwidth}}},
		every head row/.style={before row=\hline Rate & \multicolumn{9}{c}{Throughput} \\,after row=\hline},
		every last row/.style={after row=\hline},
		]{data/builtddwr/results_table.csv}
	\end{center}
\end{table}

\begin{figure}
	\caption{Shared queue and distributed database with replication - sport throughput}
	\label{figure:builtddnr_sport}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	title={Throughput against athletics demand},
	xlabel={Athletics demand (users)},
	ylabel={Throughput},
	xmin=0, xmax=150,
	ymin=0, ymax=1000,
	legend pos=north west,
	ymajorgrids=true,
	grid style=dashed,
	cycle multiindex* list={
		mark list*
		\nextlist
		cyan,brown,green,blue,red
	}
	]
	
	\addplot table [x index={0}, y index={1}, col sep=comma]{data/builtddwr/results_table.csv};
	\addplot table [x index={0}, y index={2}, col sep=comma]{data/builtddwr/results_table.csv};
	\addplot table [x index={0}, y index={5}, col sep=comma]{data/builtddwr/results_table.csv};
	
	\legend{athletics,cycling,control}
	
	\end{axis}
	\end{tikzpicture}
\end{figure}

\begin{figure}
	\caption{Shared queue and distributed database with replication - database throughput}
	\label{figure:queueddwr_database}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	title={Database throughput against athletics users},
	width=0.9\textwidth,
	ybar=0pt,
	bar width=.02\textwidth,
	xlabel={Athletics users},
	ylabel={Throughput},
	xtick=data,
	ymin=0, ymax=350,
	legend pos=north west,
	ymajorgrids=true,
	grid style=dashed,
	]
	
	\addplot [pattern=north east lines, pattern color=blue] table [x index={1}, y index={2}, col sep=comma]{data/builtddwr/average_db1.csv};
	\addplot [fill=green] table [x index={1}, y index={2}, col sep=comma]{data/builtddwr/average_db2.csv};
	\addplot [pattern=crosshatch, pattern color=brown] table [x index={1}, y index={2}, col sep=comma]{data/builtddwr/average_db3.csv};
	
	\legend{db1,db2,db3}
	
	\end{axis}
	\end{tikzpicture}
\end{figure}
