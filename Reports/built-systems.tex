%
% ---- Built systems
%

\section{Built systems}\label{sec:built-systems}

\subsection{Approach}

To test the system models, real systems using those architectures are built and their throughputs measured under simulated workloads.

\subsubsection{Designs.}  All three systems were developed in Java and deployed on Microsoft Azure virtual machines.  The system designs discuss their specifics, but the common feature is that all three systems use a Cassandra database or databases with the same ticket schema design, given in Figure \ref{figure:cassandra_ticket_schema}, with 500 tickets of each sport.

Cassandra was chosen as the distributed database models were designed for consistent hashing (most relevant for the model using replication).  Although the simple microservices model need not have used Cassandra databases, reusing the schema design and database servers was an economical choice, and it illustrates how different architectures may arrange many of the same components.

The source code for the systems and related tools is hosted on GitHub \cite{RN1073}.

\begin{figure}
	\caption{Cassandra database ticket schema}
	\label{figure:cassandra_ticket_schema}
	\centering
	\begin{lstlisting}[basicstyle=\ttfamily]
	/*
	* Ticket table schema
	* 
	* int id - unique ticket id
	* varchar sport - type of sport
	* int day - day of event
	* int seat - seat number
	* varchar owner - name of booked ticket owner
	* 
	* The partition key is sport
	* The clustering columns are owner, day, id
	*/
	
	CREATE TABLE IF NOT EXISTS ticket (
		id int,
		sport varchar,
		day int,
		seat int,
		owner varchar,
		PRIMARY KEY (sport, owner, day, id)
	) WITH comment = 'Tickets';
	\end{lstlisting}
\end{figure}

\subsubsection{Workload Simulation.}  The workload from a web application and its users was simulated using Apache JMeter \cite{RN1074}, a Java application for load testing.  JMeter was originally designed for testing web applications and all the systems have RESTful APIs over HTTP (for the simple microservices architecture, the Java Spring APIs; for the shared queue architectures, the Microsoft Azure Storage Queue REST APIs).  JMeter test plans are composed of thread groups, where the number of threads and test executions are specified, samplers such as HTTP Request, and timers to determine the delay between each test execution.  Increasing the number of threads used in the test increases the required demand.  JMeter's Poisson Random Timer is used to simulate the negative exponential distribution \cite{RN80} that PEPA steady state solutions assume.

\subsubsection{Measurement.}  Measurement of throughputs in the built systems is required to produce results that can be compared with the PEPA model experimental results.

\paragraph{Measurands.} The measurands are the throughputs of athletics and cycling requests in the worker applications, and control requests that do not access the database in order to show any system overheads.  Where applicable, measurands include the throughputs of diving requests and database queries.

\paragraph{Measurement method.}  The worker applications are measured using Coda Hale Metrics \cite{RN1079}, a Java library providing instrumentation.  The instrument used is a {\itshape Meter}, that measures the rate at which events occur as mean and moving average rates.  Database throughputs are measured by enabling the built-in Cassandra metric ThreadPools.CompletedTasks.request.ReadStage (the total count of completed read queries).  In both cases the metrics are logged every 10 seconds.

\paragraph{Measurement procedure.} The largest 1-minute moving average over a test run is extracted by a Python script.  For the worker applications, the 1-minute average is provided by the Meter instrument.  For Cassandra metric files, the average must be calculated, again using a Python script.  Each experiment is carried out five times and the mean is taken of the five sets of results.

%
% ---- Simple microservices
%
\FloatBarrier
\subsection{Simple microservices}

\subsubsection{Design.}  The simple microservices architecture from Figure \ref{figure:simplemicro_architecture} was deployed on four Azure Ubuntu virtual machines as shown in Table \ref{table:builtmicro_vmdesign}.  There are two completely separate Cassandra databases, one containing Athletics tickets, one for Cycling tickets.  There are two instances of the {\itshape SimplemicroApplication} worker application, each running on a separate virtual machine and each connecting to one of the databases.

SimplemicroApplication was built using Java Spring \cite{RN1076}, a collection of frameworks including model-view-controller (used to implement RESTful APIs for the application) and data access (which supports connection to many databases including Cassandra).  The application has two RESTful APIs:

{\textbackslash search}, which takes a sport parameter (Athletics or Cycling), queries the database for all matching tickets, and records metrics.

{\textbackslash control}, which doesn't access the database, but still records metrics.

\begin{table}[h!]
	\begin{center}
		\caption{Simple microservices Azure VMs}
		\label{table:builtmicro_vmdesign}
		\begin{tabular}{l | l | l}
			VM		& Specification		& Application \\
			\hline
			worker1	& DS1 v2 (1 core, 3.5 GB)	& SimplemicroApplication \\
			worker2	& DS1 v2 (1 core, 3.5 GB)	& SimplemicroApplication \\
			db1		& F1s (1 core, 2 GB)	& Cassandra \\
			db2		& F1s (1 core, 2 GB)	& Cassandra \\
		\end{tabular}
	\end{center}
\end{table}

\subsubsection{Workload Simulation.}  The application was tested using JMeter with two thread groups running in parallel, Cycling with a constant 10 threads (users) and Athletics ramping up from 10-100 in steps of 10.  Each thread group had a Poisson random timer with a lambda value of 500 milliseconds and a loop count of 500 requests, ensuring several minutes worth of samples and therefore a number of rolling 1-minute averages.  The test plan was used against the {\textbackslash control} APIs for tuning and calibration, showing that the constant demand was approximately 19 requests per second and the skewed demand ranged from 19-190 requests per second (again, approximately, given that a the distribution is random). The test plan was then applied to the {\textbackslash search} API with the same parameters.

\subsubsection{Results.}  The results are given numerically in Table \ref{table:builtmicro_results} and shown graphically in Figure \ref{figure:builtmicro_charts} (where the `control' plot is the control throughput from the athletics virtual machine).  The athletics worker throughput using {\textbackslash search} was measured to be limited to 130 queries per second, compared to the control throughput on the same virtual machine i.e. that is the limit imposed by the database.  Using Cassandra's packaged cassandra-stress load testing tool on the database server shows that it is capable of much higher query rate, which suggests that using Java Spring Data adds overheads to the database requests (perhaps, given the use of sessionless RESTful APIs, the main overhead is setting up a connection session to Cassandra each time).  The PEPA database model has already been acknowledged to be an abstraction which does not include such factors, and the scaled rate was fed back into the model's database service rate.

The cycling throughput is more isolated from athletics than the queue-based systems, but there is a slow downward trend as athletics demand increases.  The only point at which the two microservices connect is at JMeter, so this may be the effect of increased demand on JMeter itself, although the control throughput results from the cycling virtual machine do not clearly demonstrate the same trend.  Another potential cause is co-residency of the virtual machines.

\begin{table}[h!]
	\begin{center}
		\caption{Simple microservices experimental results}
		\label{table:builtmicro_results}
		\pgfplotstabletypeset[
		col sep=comma,
		string type,
		columns/ausers/.style={column name=users, column type={p{.15\textwidth}}},
		columns/acontrol/.style={column name=control, column type={p{.15\textwidth}}},
		columns/athletics/.style={column name=search, column type={p{.15\textwidth}}},
		columns/cusers/.style={column name=users, column type={p{.15\textwidth}}},
		columns/ccontrol/.style={column name=control, column type={p{.15\textwidth}}},
		columns/cycling/.style={column name=search, column type={p{.15\textwidth}}},
		every head row/.style={before row=\hline \multicolumn{3}{c}{Athletics worker} & \multicolumn{3}{c}{Cycling worker} \\,after row=\hline},
		every last row/.style={after row=\hline},
		]{data/builtmicro/results.csv}
	\end{center}
\end{table}

\begin{figure}
	\caption{Simple microservices experimental results}
	\label{figure:builtmicro_charts}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	title={Throughput against athletics demand},
	xlabel={Athletics demand (users)},
	ylabel={Throughput},
	xmin=0, xmax=100,
	ymin=0, ymax=200,
	legend pos=north west,
	ymajorgrids=true,
	grid style=dashed,
	cycle multiindex* list={
		mark list*
		\nextlist
		cyan,brown,green,blue,red
	}
	]

	\addplot table [x index={0}, y index={2}, col sep=comma]{data/builtmicro/results.csv};
	\addplot table [x index={0}, y index={5}, col sep=comma]{data/builtmicro/results.csv};
	\addplot table [x index={0}, y index={1}, col sep=comma]{data/builtmicro/results.csv};
	
	\legend{athletics,cycling,control}
	
	\end{axis}
	\end{tikzpicture}
\end{figure}

%
% ---- Shared queue middleware
%
\FloatBarrier
\subsection{Shared queue and distributed database}

\subsubsection{Design.}  The shared queue and distributed database architecture from Figure \ref{figure:queuedd_architecture} was deployed on three Azure Ubuntu virtual machines as shown in Table \ref{table:builtddnr_vmdesign}, and using a single Azure Storage Queue instance.  Azure queues provide their own RESTful API access and JMeter was used to populate the queue with Athletics, Cycling or Control ticket requests.

The {\itshape QueueWorker} application was built as a multithreaded application running on a single, multi-core virtual machine.  It dequeues every request (Athletics, Cycling or Control tickets) from the shared Azure Storage Queue.  This asynchronous middleware would be best suited to the `return ticket' operation from the use case, but to ensure that a usable Cassandra metric was available for measuring database throughput, an Athletics or Cycling request was interpreted as a `search ticket` operation.  A database select of all tickets for the matching sport is carried out first and the metric is recorded if the query returns results.  As before the Control request does not access the database but records metrics.  When connecting to the Cassandra database, the QueueWorker application uses a round-robin algorithm to select the coordinator node, to ensure that any additional overheads incurred are shared equally between both nodes.

As populating the queue with JMeter and processing it with the queue worker application are decoupled, it was possible to run QueueWorker on a prepopulated queue to determine its maximum throughput i.e. regardless of the incoming demand.  Using this technique suggested that maximum performance came with QueueWorker running with 16 threads.

The Cassandra database was distributed using a keyspace (`Distributed') with SimpleStrategy, replication\_factor=1 onto two nodes each on one virtual machine.  Cassandra's partitioning places Athletics tickets on one node, and Cycling tickets on the other.  This was validated using nodetool getendpoints, which shows which node hosts records from a given keyspace matching a given partition key e.g. nodetool getendpoints Distributed ticket Athletics.

Note that without the overheads of starting a new Cassandra database session for each request, it was necessary to slow Cassandra down by turning on tracing for 100\% of queries using nodetool settraceprobability 1.0.  Load testing the database alone with cassandra-stress using 16 threads and a query for all athletics tickets suggested a maximum database throughput of 475 operations per second.

\begin{table}[h!]
	\begin{center}
		\caption{Shared queue and distributed DB Azure VMs}
		\label{table:builtddnr_vmdesign}
		\begin{tabular}{l | l | l}
			VM		& Specification			& Application \\
			\hline
			qworkers	& DS3 v2 Promo (4 cores, 14 GB)	& QueueWorker (16 threads) \\
			db1		& F1s (1 core, 2 GB)		& Cassandra \\
			db2		& F1s (1 core, 2 GB)		& Cassandra \\
		\end{tabular}
	\end{center}
\end{table}

\subsubsection{Workload Simulation.}  The application was tested using JMeter with two thread groups running in parallel, Cycling with a constant 15 threads and Athletics ramping up from 15-150 in steps of 15.  Each thread group had a Poisson random timer with a lambda value of 100 milliseconds and a loop count of 1500 requests, ensuring enough samples for a number of rolling 1-minute averages.  Again there was a control version of test plan with the same variables, but sending Control ticket requests to the queue rather than Athletics and Cycling tickets.  This produces constant demand of approximately 95 requests per second and skewed demand from 95-950 requests per second.  Scaling these results with the cassandra-stress measurement about to the model input range of 1-10 suggested keeping the {\itshape db} rate at 5.0.

\subsubsection{Results.} 
Table \ref{table:builtddnr_results} shows the numerical results and Figure \ref{figure:builtddnr_charts} shows the plots of athletics, cycling and control throughputs for the same range of athletics demands stated as the number of JMeter threads or users.

The total throughput of {\itshape athletics} and {\itshape cycling} quickly falls behind the control throughput, and examination of the athletics throughput in particular shows that it is constrained by the database performance of a single node.  Note that the constrained throughout is significantly less than the maximum rate shown by cassandra-stress.  Interestingly the maximum total throughput is still less than that value but much closer to it, so the limit shown by cassandra-stress may be based on the whole cluster even though the tickets only appear on one node.

The {\itshape cycling} throughput is constrained in proportion to the athletics {\itshape demand}, not the actual athletics throughput, so that as the athletics throughput is limited but the demand continues to decrease, then the cycling throughput decreases in the ratio of their respective demands (though the ratio is not exact).  The behaviour of the built system matches the prediction of the model.  The model had a very small queue size, and an actual Azure Storage Queue is practically unlimited, so this result was not certain.

Finally the database node throughput follows the throughput of each sport activity.  The partitioning strategy placed the athletics tickets onto {\itshape db2}, as discovered using nodetool getendpoints, and the throughput on this node increases with the skewed athletics demand.

\begin{table}[h!]
	\begin{center}
		\caption{Shared queue and distributed DB experimental results}
		\label{table:builtddnr_results}
		\pgfplotstabletypeset[
		col sep=comma,
		string type,
		columns/ausers/.style={column name=(users), column type={p{.1\textwidth}}},
		columns/arate/.style={column name=athletics, column type={p{.1\textwidth}}},
		columns/crate/.style={column name=cycling, column type={p{.1\textwidth}}},
		columns/total/.style={column name=total, column type={p{.1\textwidth}}},
		columns/conrate/.style={column name=control, column type={p{.1\textwidth}}},
		columns/ratio/.style={column name=ratio, column type={p{.1\textwidth}}},
		columns/db1/.style={column name=db1, column type={p{.1\textwidth}}},
		columns/db2/.style={column name=db2, column type={p{.1\textwidth}}},
		every head row/.style={before row=\hline Rate & \multicolumn{7}{c}{Throughput} \\,after row=\hline},
		every last row/.style={after row=\hline},
		]{data/builtddnr/results_table.csv}
	\end{center}
\end{table}

\begin{figure}
	\caption{Shared queue and distributed DB experimental results}
	\label{figure:builtddnr_charts}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	title={Throughput against athletics demand},
	xlabel={Athletics demand (users)},
	ylabel={Throughput},
	xmin=0, xmax=150,
	ymin=0, ymax=1000,
	legend pos=north west,
	ymajorgrids=true,
	grid style=dashed,
	cycle multiindex* list={
		mark list*
		\nextlist
		cyan,brown,green,blue,red
	}
	]

	\addplot table [x index={0}, y index={1}, col sep=comma]{data/builtddnr/results_table.csv};
	\addplot table [x index={0}, y index={2}, col sep=comma]{data/builtddnr/results_table.csv};
	\addplot table [x index={0}, y index={4}, col sep=comma]{data/builtddnr/results_table.csv};
	
	\legend{athletics,cycling,control}
	
	\end{axis}
	\end{tikzpicture}
\end{figure}

%
% ---- Distributed database with replication
%
\FloatBarrier
\subsection{Shared queue and distributed database with replication}

\subsubsection{Design.}  The shared queue and distributed database with replication architecture from Figure \ref{figure:queueddrep_architecture} was deployed on four Azure Ubuntu virtual machines as shown in Table \ref{table:builtddwr_vmdesign}, again using a single Azure Storage Queue instance.

The {\itshape QueueWorker} application was reused (as well as handling Athletics, Cycling or Control ticket requests, the application handles Diving requests) and run with 16 threads as before.

The Cassandra database was distributed using a keyspace (`Replicated') with SimpleStrategy, replication\_factor=2 onto three nodes each on one virtual machine.  Cassandra's partitioning places Athletics, Cycling and Diving tickets on different nodes with each node also containing replicas of one other ticket type (it was necessary to use ByteOrderedPartitioner to force this).  This is verified using nodetool getendpoints e.g. nodetool getendpoints Distributed ticket Athletics.

\begin{center}
\begin{tabular}{l | l | l}
	Node	& Primary	& Replica \\
	\hline
	db1		& Athletics	& Cycling \\
	db2		& Cycling	& Diving \\
	db3		& Diving	& Athletics \\
\end{tabular}
\end{center}

Again, it was necessary to slow Cassandra down by turning on tracing for 100\% of queries.

Load testing the database with cassandra-stress using 16 threads suggested a maximum database throughput of 600 operations per second.  This suggests that replicating tickets on two database nodes increases the possible throughput, but it hasn't doubled the previous performance of 475 ops per second.  The model keeps the {\itshape db} rate of 5.0 based on that 475 ops per second single-node measurement.

\begin{table}[h!]
	\begin{center}
		\caption{Shared queue and distributed DB with replication Azure VMs}
		\label{table:builtddwr_vmdesign}
		\begin{tabular}{l | l | l}
			VM		& Specification			& Application \\
			\hline
			qworkers	& DS3 v2 Promo (4 cores, 14 GB)	& QueueWorker (16 threads) \\
			db1		& F1s (1 core, 2 GB)		& Cassandra \\
			db2		& F1s (1 core, 2 GB)		& Cassandra \\
			db3		& F1s (1 core, 2 GB)		& Cassandra \\
		\end{tabular}
	\end{center}
\end{table}

\subsubsection{Workload Simulation.}  The application was tested using JMeter with three thread groups running in parallel, Cycling and Diving with a constant 15 threads and Athletics ramping up from 15-150 in steps of 15.  The parameters were set to the same values as the previous JMeter test plan.

\subsubsection{Results.} 
Table \ref{table:builtddwr_results} shows the numerical results. Figure \ref{figure:builtddwr_sport} shows the plots of athletics, cycling and control throughputs for the same range of athletics demands.  As for the model, diving throughput is not shown on the chart as it closely matches cycling throughput.  Figure \ref{figure:builtddwr_database} shows the database throughputs for all three nodes.

The control throughput begins to taper off as it gets closer to 1000 requests per second.  Azure queues have a limit of up to 2000 messages per second, so simultaneously queueing and dequeuing requests at the same rate suggests a limit of 1000 messages per second.  The total throughput of {\itshape athletics}, {\itshape cycling} and {\itshape diving} still falls behind control throughput, and again the maximum total is very close to the cassandra-stress limit of 600 requests per second.  Both the total and athletics throughputs are significantly higher than for the system without replication, as predicted by the model.

Again the model correctly predicted that the {\itshape cycling} and {\itshape diving} throughputs would be constrained in a ratio to the athletics throughput that matches (or is close to) the ratio between their demands.

However, the database node throughputs are very different to those from the model.  While the {\itshape db1} and {\itshape db3} nodes that host athletics tickets do show higher throughputs than the node which does not, the throughputs were not shared equally between those two nodes.  Neither was the allocation random.  The average of all five sets of tests showed {\itshape db3} taking many more requests.  This, and the cassandra-stress result showing that the athletics query performance did not double when adding a replica to another node, indicates that Cassandra's performance is more complex than the abstract database model.

\begin{table}[h!]
	\begin{center}
		\caption{Shared queue and distributed database with replication experimental results}
		\label{table:builtddwr_results}
		\pgfplotstabletypeset[
		col sep=comma,
		string type,
		columns/ausers/.style={column name=(users), column type={p{.1\textwidth}}},
		columns/arate/.style={column name=athletics, column type={p{.1\textwidth}}},
		columns/crate/.style={column name=cycling, column type={p{.1\textwidth}}},
		columns/drate/.style={column name=diving, column type={p{.1\textwidth}}},
		columns/total/.style={column name=total, column type={p{.1\textwidth}}},
		columns/conrate/.style={column name=control, column type={p{.1\textwidth}}},
		columns/ratio/.style={column name=ratio, column type={p{.1\textwidth}}},
		columns/db1/.style={column name=db1, column type={p{.1\textwidth}}},
		columns/db2/.style={column name=db2, column type={p{.1\textwidth}}},
		columns/db3/.style={column name=db3, column type={p{.1\textwidth}}},
		every head row/.style={before row=\hline Rate & \multicolumn{9}{c}{Throughput} \\,after row=\hline},
		every last row/.style={after row=\hline},
		]{data/builtddwr/results_table.csv}
	\end{center}
\end{table}

\begin{figure}
	\caption{Shared queue and distributed database with replication - sport throughput}
	\label{figure:builtddwr_sport}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	title={Throughput against athletics demand},
	xlabel={Athletics demand (users)},
	ylabel={Throughput},
	xmin=0, xmax=150,
	ymin=0, ymax=1000,
	legend pos=north west,
	ymajorgrids=true,
	grid style=dashed,
	cycle multiindex* list={
		mark list*
		\nextlist
		cyan,brown,green,blue,red
	}
	]
	
	\addplot table [x index={0}, y index={1}, col sep=comma]{data/builtddwr/results_table.csv};
	\addplot table [x index={0}, y index={2}, col sep=comma]{data/builtddwr/results_table.csv};
	\addplot table [x index={0}, y index={5}, col sep=comma]{data/builtddwr/results_table.csv};
	
	\legend{athletics,cycling,control}
	
	\end{axis}
	\end{tikzpicture}
\end{figure}

\begin{figure}
	\caption{Shared queue and distributed database with replication - database throughput}
	\label{figure:builtddwr_database}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	title={Database throughput against athletics users},
	width=0.9\textwidth,
	ybar=0pt,
	bar width=.02\textwidth,
	xlabel={Athletics users},
	ylabel={Throughput},
	xtick=data,
	ymin=0, ymax=350,
	legend pos=north west,
	ymajorgrids=true,
	grid style=dashed,
	]
	
	\addplot [pattern=north east lines, pattern color=blue] table [x index={1}, y index={2}, col sep=comma]{data/builtddwr/average_db1.csv};
	\addplot [fill=green] table [x index={1}, y index={2}, col sep=comma]{data/builtddwr/average_db2.csv};
	\addplot [pattern=crosshatch, pattern color=brown] table [x index={1}, y index={2}, col sep=comma]{data/builtddwr/average_db3.csv};
	
	\legend{db1,db2,db3}
	
	\end{axis}
	\end{tikzpicture}
\end{figure}
